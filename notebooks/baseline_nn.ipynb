{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rootutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "rootutils.setup_root(Path.cwd(), indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.trainer import Trainer  # noqa: E402\n",
    "from src.utils import EarlyStopping  # noqa: E402\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_std = 1e-8\n",
    "train_val_split = (0.8, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.read_csv(\"../data/train.csv\", nrows=1).columns.to_list()\n",
    "features = columns[1:557]\n",
    "targets = columns[557:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XScaler:\n",
    "    def __init__(self, min_std=1e-8):\n",
    "        self.mean: np.ndarray\n",
    "        self.std: np.ndarray\n",
    "        self.min_std = min_std\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> None:\n",
    "        self.mean = X.mean(axis=0)\n",
    "        self.std = np.maximum(X.std(axis=0), self.min_std)\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = (X - self.mean.reshape(1, -1)) / self.std.reshape(1, -1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YScaler:\n",
    "    def __init__(self, min_std=1e-8):\n",
    "        self.mean: np.ndarray\n",
    "        self.s: np.ndarray\n",
    "        self.min_std = min_std\n",
    "\n",
    "    def fit(self, y: np.ndarray):\n",
    "        self.mean = y.mean(axis=0)\n",
    "        self.s = np.maximum(np.sqrt((y * y).mean(axis=0)), self.min_std)\n",
    "\n",
    "    def transform(self, y: np.ndarray) -> np.ndarray:\n",
    "        y = (y - self.mean.reshape(1, -1)) / self.s.reshape(1, -1)\n",
    "        return y\n",
    "\n",
    "    def inverse_transform(self, y: np.ndarray) -> np.ndarray:\n",
    "        # override constant columns\n",
    "        for i in range(self.s.shape[0]):\n",
    "            if self.s[i] < self.min_std * 1.1:\n",
    "                y[:, i] = 0\n",
    "        # undo y scaling\n",
    "        y = y * self.s.reshape(1, -1) + self.mean.reshape(1, -1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir: Path, n_rows=10_000) -> tuple[np.ndarray, ...]:\n",
    "    df = pd.read_csv(\n",
    "        data_dir.joinpath(\"train.csv\"), nrows=n_rows, usecols=list(range(1, 925))\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    nunique_targets = df[features].nunique().to_numpy()\n",
    "\n",
    "    weights = pd.read_csv(\n",
    "        data_dir.joinpath(\"sample_submission.csv\"), nrows=1, usecols=list(range(1, 369))\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    weights = weights.to_numpy().reshape(1, -1)\n",
    "\n",
    "    X = df[features].to_numpy()\n",
    "    y = df[targets].to_numpy() * weights\n",
    "\n",
    "    train_size = int(len(X) * train_val_split[0])\n",
    "\n",
    "    X_train, y_train = X[:train_size, :], y[:train_size, :]\n",
    "\n",
    "    X_val, y_val = X[train_size:, :], y[train_size:, :]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, nunique_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, nunique_targets = read_data(\n",
    "    Path(\"../data/\"), n_rows=1_000_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xscaler = XScaler()\n",
    "xscaler.fit(X_train)\n",
    "X_train = xscaler.transform(X_train)\n",
    "X_val = xscaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.absolute(y_train).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n",
    "        super().__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
    "        dilation_rates = [2**i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=int((dilation_rate * (kernel_size - 1)) / 2),\n",
    "                    dilation=dilation_rate,\n",
    "                )\n",
    "            )\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=int((dilation_rate * (kernel_size - 1)) / 2),\n",
    "                    dilation=dilation_rate,\n",
    "                )\n",
    "            )\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(\n",
    "                self.gate_convs[i](x)\n",
    "            )\n",
    "            x = self.convs[i + 1](x)\n",
    "            res = res + x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, output_size, inch=9, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.wave_block1 = WaveBlock(inch, 16, 12, kernel_size)\n",
    "        self.wave_block2 = WaveBlock(16, 32, 8, kernel_size)\n",
    "        self.wave_block3 = WaveBlock(32, 64, 4, kernel_size)\n",
    "        self.wave_block4 = WaveBlock(64, 128, 1, kernel_size)\n",
    "        self.lstm = nn.LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc_in = nn.Linear(60 * 128 + 16, 1024)\n",
    "        self.fc_out = nn.Linear(1024, output_size)\n",
    "        self._reinitialize()\n",
    "\n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if \"lstm\" in name:\n",
    "                if \"weight_ih\" in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif \"weight_hh\" in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif \"bias_ih\" in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4) : (n // 2)].fill_(1)\n",
    "                elif \"bias_hh\" in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif \"fc\" in name:\n",
    "                if \"weight\" in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif \"bias\" in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_seq = torch.cat((x[:, :360], x[:, -180:]), dim=1).view(x.shape[0], 9, 60)\n",
    "        x_scalar = x[:, 360:376]\n",
    "\n",
    "        x = self.wave_block1(x_seq)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "        x = self.wave_block4(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = torch.cat((x, x_scalar), dim=1)\n",
    "        x = nn.functional.silu(self.fc_in(x))\n",
    "        x = nn.functional.dropout(x, p=0.2)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the layers\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            layers.append(nn.LayerNorm(hidden_size))  # Normalization layer\n",
    "            layers.append(nn.LeakyReLU(inplace=True))  # Activation\n",
    "            layers.append(nn.Dropout(p=0.1))  # Dropout for regularization\n",
    "            previous_size = hidden_size\n",
    "\n",
    "        # Output layer - no dropout, no activation function\n",
    "        layers.append(nn.Linear(previous_size, output_size))\n",
    "\n",
    "        # Register all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Initialize with NumPy arrays.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            x.shape[0] == y.shape[0]\n",
    "        ), \"Features and labels must have the same number of samples\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples.\n",
    "        \"\"\"\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one sample of data.\n",
    "        \"\"\"\n",
    "        # Convert the data to tensors when requested\n",
    "        return torch.from_numpy(self.x[index]), torch.from_numpy(self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "val_dataset = NumpyDataset(X_val, y_val)\n",
    "\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "#     dataset, train_val_split, generator=torch.Generator().manual_seed(42)\n",
    "# )\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=10\n",
    ")\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "hidden_size = input_size + output_size\n",
    "\n",
    "model = WaveNet(output_size)\n",
    "\n",
    "# model = FFNN(\n",
    "#     input_size,\n",
    "#     [3 * hidden_size, 2 * hidden_size, hidden_size, 2 * hidden_size, 3 * hidden_size],\n",
    "#     output_size,\n",
    "# )\n",
    "\n",
    "criterion = nn.MSELoss()  # Using MSE for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=5\n",
    ")\n",
    "\n",
    "\n",
    "def preprocessor(\n",
    "    y_pred: np.ndarray,\n",
    "    y_true: np.ndarray,\n",
    "    # y_mean: np.ndarray = y_mean,\n",
    "    # nunique: np.ndarray = nunique_targets,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    scores = r2_score(y_true, y_pred, multioutput=\"raw_values\")\n",
    "\n",
    "    for idx, score in enumerate(scores):  # type: ignore\n",
    "        if score <= 0:\n",
    "            # if score <= 0 or (nunique[idx] == 1 and y_mean[idx] <= 1e-3):\n",
    "            y_pred[:, idx] = 0\n",
    "\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=10, verbose=True, delta=0.0, on_each_epoch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.4987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (inf --> 0.406775).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 122s 19ms/step - loss: 0.4987 - r2: 0.4974 - val_loss: 0.4068 - val_r2: 0.5408 - lr: 1.0000e-03\n",
      "Epoch: 2/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.4256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.406775 --> 0.380290).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 124s 20ms/step - loss: 0.4256 - r2: 0.5666 - val_loss: 0.3803 - val_r2: 0.5660 - lr: 1.0000e-03\n",
      "Epoch: 3/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3989"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.380290 --> 0.369800).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.3989 - r2: 0.5916 - val_loss: 0.3698 - val_r2: 0.5766 - lr: 1.0000e-03\n",
      "Epoch: 4/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 124s 20ms/step - loss: 0.3818 - r2: 0.6081 - val_loss: 0.3725 - val_r2: 0.5737 - lr: 1.0000e-03\n",
      "Epoch: 5/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.3659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.3659 - r2: 0.6229 - val_loss: 0.3730 - val_r2: 0.5742 - lr: 1.0000e-03\n",
      "Epoch: 6/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.369800 --> 0.362186).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.3506 - r2: 0.6374 - val_loss: 0.3622 - val_r2: 0.5835 - lr: 1.0000e-03\n",
      "Epoch: 7/100\n",
      "6246/6250 [========>.] - ETA: 0s - loss: 0.3390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.362186 --> 0.357999).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.3390 - r2: 0.6480 - val_loss: 0.3580 - val_r2: 0.5878 - lr: 1.0000e-03\n",
      "Epoch: 8/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.3254 - r2: 0.6608 - val_loss: 0.3584 - val_r2: 0.5873 - lr: 1.0000e-03\n",
      "Epoch: 9/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.3157 - r2: 0.6698 - val_loss: 0.3626 - val_r2: 0.5851 - lr: 1.0000e-03\n",
      "Epoch: 10/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.3071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.357999 --> 0.357835).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.3071 - r2: 0.6781 - val_loss: 0.3578 - val_r2: 0.5864 - lr: 1.0000e-03\n",
      "Epoch: 11/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.2969 - r2: 0.6876 - val_loss: 0.3592 - val_r2: 0.5874 - lr: 1.0000e-03\n",
      "Epoch: 12/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.2888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 124s 20ms/step - loss: 0.2888 - r2: 0.6952 - val_loss: 0.3582 - val_r2: 0.5860 - lr: 1.0000e-03\n",
      "Epoch: 13/100\n",
      "6246/6250 [========>.] - ETA: 0s - loss: 0.2825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.2825 - r2: 0.7009 - val_loss: 0.3661 - val_r2: 0.5794 - lr: 1.0000e-03\n",
      "Epoch: 14/100\n",
      "6246/6250 [========>.] - ETA: 0s - loss: 0.2772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 126s 20ms/step - loss: 0.2772 - r2: 0.7061 - val_loss: 0.3637 - val_r2: 0.5807 - lr: 1.0000e-03\n",
      "Epoch: 15/100\n",
      "6247/6250 [========>.] - ETA: 0s - loss: 0.2702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.2702 - r2: 0.7128 - val_loss: 0.3590 - val_r2: 0.5867 - lr: 1.0000e-03\n",
      "Epoch: 16/100\n",
      "6246/6250 [========>.] - ETA: 0s - loss: 0.2646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 124s 20ms/step - loss: 0.2646 - r2: 0.7178 - val_loss: 0.3637 - val_r2: 0.5805 - lr: 1.0000e-04\n",
      "Epoch: 17/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.2341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "Validation loss decreased (0.357835 --> 0.349194).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 122s 20ms/step - loss: 0.2341 - r2: 0.7468 - val_loss: 0.3492 - val_r2: 0.5957 - lr: 1.0000e-04\n",
      "Epoch: 18/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2260"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.2260 - r2: 0.7544 - val_loss: 0.3496 - val_r2: 0.5954 - lr: 1.0000e-04\n",
      "Epoch: 19/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.2222 - r2: 0.7579 - val_loss: 0.3502 - val_r2: 0.5948 - lr: 1.0000e-04\n",
      "Epoch: 20/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.2196 - r2: 0.7604 - val_loss: 0.3512 - val_r2: 0.5942 - lr: 1.0000e-04\n",
      "Epoch: 21/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 120s 19ms/step - loss: 0.2176 - r2: 0.7622 - val_loss: 0.3532 - val_r2: 0.5922 - lr: 1.0000e-04\n",
      "Epoch: 22/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.2158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 122s 19ms/step - loss: 0.2158 - r2: 0.7640 - val_loss: 0.3532 - val_r2: 0.5916 - lr: 1.0000e-04\n",
      "Epoch: 23/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.2143 - r2: 0.7654 - val_loss: 0.3532 - val_r2: 0.5919 - lr: 1.0000e-05\n",
      "Epoch: 24/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.2101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 122s 19ms/step - loss: 0.2101 - r2: 0.7694 - val_loss: 0.3535 - val_r2: 0.5916 - lr: 1.0000e-05\n",
      "Epoch: 25/100\n",
      "6249/6250 [========>.] - ETA: 0s - loss: 0.2097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 123s 20ms/step - loss: 0.2097 - r2: 0.7698 - val_loss: 0.3534 - val_r2: 0.5916 - lr: 1.0000e-05\n",
      "Epoch: 26/100\n",
      "6246/6250 [========>.] - ETA: 0s - loss: 0.2095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 124s 20ms/step - loss: 0.2095 - r2: 0.7700 - val_loss: 0.3532 - val_r2: 0.5919 - lr: 1.0000e-05\n",
      "Epoch: 27/100\n",
      "6248/6250 [========>.] - ETA: 0s - loss: 0.2093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/utils.py:\n",
      "EarlyStopping counter: 10 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==========] - 121s 19ms/step - loss: 0.2093 - r2: 0.7702 - val_loss: 0.3531 - val_r2: 0.5920 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/projects/kaggle/LEAP/src/trainer.py:Early stopping\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    loss_func=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=100,\n",
    "    score_funcs={\"r2\": r2_score},\n",
    "    device=device,\n",
    "    checkpoint_dir=Path(\"../logs/\"),\n",
    "    postprocessor=preprocessor,\n",
    "    early_stopping=early_stopping,\n",
    "    lr_scheduler=scheduler,\n",
    ")\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(ckpt_dir: Path) -> tuple[np.ndarray, np.ndarray]:\n",
    "    state_dict = torch.load(f\"{ckpt_dir.joinpath(model.__class__.__name__)}_best.ckpt\")[\n",
    "        \"model_state_dict\"\n",
    "    ]\n",
    "    model.to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            preds.append(model(inputs.to(device)).detach().cpu().numpy())\n",
    "            targets.append(labels.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    return preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [00:09<00:00, 168.92it/s]\n"
     ]
    }
   ],
   "source": [
    "val_preds, val_targets = eval(ckpt_dir=Path(\"../logs/2024-05-09_20-55-17\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.887160960117868e+40"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(val_targets, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_r2_scores = r2_score(val_targets, val_preds, multioutput=\"raw_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, score in enumerate(val_r2_scores):  # type: ignore\n",
    "    if score <= 0:\n",
    "        val_preds[:, idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5984239527794785"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(val_targets, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ckpt_dir: Path, filename: str) -> np.ndarray:\n",
    "    state_dict = torch.load(f\"{ckpt_dir.joinpath(model.__class__.__name__)}_best.ckpt\")[\n",
    "        \"model_state_dict\"\n",
    "    ]\n",
    "    model.to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    X = pd.read_csv(filename)[features].astype(\"float32\").to_numpy()\n",
    "\n",
    "    X = xscaler.transform(X)\n",
    "\n",
    "    test_dataset = NumpyDataset(X, np.zeros((X.shape[0], output_size)))\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(test_loader):\n",
    "            preds.append(model(inputs.to(device)).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [00:23<00:00, 210.22it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(\n",
    "    ckpt_dir=Path(\"../logs/2024-05-09_20-55-17\"), filename=\"../data/test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(\n",
    "    data_dir: Path,\n",
    "    y: np.ndarray,\n",
    "    val_r2_scores: np.ndarray,\n",
    "    # y_mean: np.ndarray = y_mean,\n",
    "    # nunique: np.ndarray = nunique_targets,\n",
    ") -> np.ndarray:\n",
    "    weights = (\n",
    "        pd.read_csv(\n",
    "            data_dir.joinpath(\"sample_submission.csv\"),\n",
    "            nrows=1,\n",
    "            usecols=list(range(1, 369)),\n",
    "        )\n",
    "        .astype(\"float32\")\n",
    "        .to_numpy()\n",
    "        .reshape(1, -1)\n",
    "    )\n",
    "\n",
    "    for idx, weight in enumerate(weights[0]):\n",
    "        if weight > 0:\n",
    "            continue\n",
    "        else:\n",
    "            y[:, idx] = 0\n",
    "\n",
    "    for idx, score in enumerate(val_r2_scores):  # type:ignore\n",
    "        if score <= 0:\n",
    "        # if score <= 0 or (nunique[idx] == 1 and y_mean[idx] <= 1e-3):\n",
    "            y[:, idx] = 0\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = postprocessing(\n",
    "    data_dir=Path(\"../data/\"),\n",
    "    y=preds,\n",
    "    val_r2_scores=val_r2_scores,  # type:ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(data_dir: Path, y: np.ndarray) -> None:\n",
    "    samples_submission = pd.read_csv(data_dir.joinpath(\"sample_submission.csv\"))\n",
    "    samples_submission.iloc[:, 1:] = y\n",
    "    samples_submission.to_parquet(\n",
    "        data_dir.joinpath(\"submission_wavenet_lstm_10_percent_of_data.parquet\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(Path(\"../data/\"), preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
