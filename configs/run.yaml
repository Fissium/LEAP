dataset_root: /projects/kaggle/LEAP/data/
logs_root: /projects/kaggle/LEAP/logs/
test_filename: test.csv
ss_filename: sample_submission.csv

hydra_dir: ${logs_root}/${now:%Y-%m-%d}_${now:%H-%M-%S}

seed: 42

dataset:
  n_rows: all # must be divisible by 200_000 or all
  train_val_split: [0.9, 0.1]
  num_features: 19
  num_targets: 368

early_stopping:
  _target_: src.utils.EarlyStopping
  patience: 15
  delta: 0.0
  verbose: True
  on_each_epoch: False

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 1e-3
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: max
  factor: 0.1
  patience: 10

model:
  _target_: src.models.transformerv2.Model
  in_chans: 19
  out_chans: 13
  max_len: 60
  embed_dim: 192
  embed_layer: fc
  head: mlp
  ffn_layer: swiglu
  depth: 12
  num_heads: 6
  mlp_ratio: 2.0
  qkv_bias: True
  ffn_bias: True
  proj_bias: True
  drop_path_rate: 0.0
  drop_path_uniform: False

criterion:
  _target_: torch.nn.L1Loss

criterion_delta:
  _target_: torch.nn.L1Loss

trainer:
  epochs: 1
  checkpoint_dir: ${hydra_dir}
  norm_value: 1.0
  batch_size: 256
  num_workers: 8

hydra:
  run:
    dir: ${hydra_dir}
  job:
    chdir: True
